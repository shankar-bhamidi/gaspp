---
title: "STOR 89X (Spring 2026): GASPp"
---

# STOR 89X (Spring 2026): GASPp  
## Generative AI, Statistical and Probabilistic (p)rinciples

**Instructor:** Shankar Bhamidi  
**Meeting time:** Tue–Thu, 11:00–12:15  
**Location:** Hanes 107  

> **Note on course logistics:** Official course materials, announcements, submissions, and grades will be handled via **UNC Canvas**.  
> This public website provides a lightweight “front door” with links to slides/notes/readings (e.g., via Dropbox) and a running schedule.

---

## Course overview

This course explores Generative AI through the lens of probability and statistics, with a particular emphasis on identifying—or questioning—the existence of coherent statistical and probabilistic principles underlying modern generative models.

The course title **GASPp** reflects this goal:
- **GASP**: Generative AI, Statistical and probabilistic …
- **p** (lower case): principles — an open question (still emerging vs. clean/unified)

This is an intentionally exploratory, research-oriented course. Roughly two-thirds of the semester will focus on building foundational understanding of:
- Large Language Models (LLMs)
- Diffusion models
- Related optimization and representation-learning frameworks

The course is not implementation-heavy; the emphasis is on understanding, intuition, reading research papers critically, and identifying research directions.

## Official syllabus

More details can be [found here](https://www.dropbox.com/scl/fi/8bz46six7z77tcvzb74zq/syllabus.pdf?rlkey=4r3ei83i5k3orujxzjo3zvwda&dl=0)

### Major acknowledgement

I can only convey my understanding of the material, much of which I learnt from the material in the [Resources](resources). In particular I need to mention the wonderful resources developed by [Prof. Cosma Shalizi](https://www.stat.cmu.edu/~cshalizi/genai/25/) and [Prof. Ambuj Tewari](https://www.ambujtewari.com/LLM-fall2024/). I am largely following in the shadow of these giants. 

---

## Learning goals

By the end of the semester, students should:
- Be fluent in the conceptual and mathematical language of modern generative models.
- Understand how probability/statistics/stochastic-process ideas interact with (and often fail to explain) these models.
- Be able to read, dissect, and present current research papers.
- Identify a concrete research direction or question with the potential to evolve into a paper.

---

## Quick links

- **[Schedule](schedule)** (main, living schedule + materials links). **This is probably the most important part of this webpage**
- **[Resources](resources)** (books, notes, online courses)
- **[Policies & Projects](policies)** (expectations, proposal, final project)

---

## Announcements

- Brief course updates here; Canvas remains the source of truth for logistics!

---

## Tentative schedule (initial part; subject to change)

> The *living* version with Dropbox links lives on the **[Schedule](schedule)** page.

| Date | Day | Topic |
|---|---|---|
| 2026-01-08 | Thu | Intro to course. Class expectations. |
| 2026-01-13 | Tue | Basics of text mining one needs: BPE (tokens) / N-grams / vector semantics / representation learning |
| 2026-01-15 | Thu | Information theory: cross-entropy loss and MLE |
| 2026-01-20 | Tue | Information theory: AEP, basic ergodic theory, Shannon–McMillan–Breiman theorem |
| 2026-01-22 | Thu | Baby text mining: fitting Markov chains; stochastic gradient descent |
| 2026-01-27 | Tue | Stochastic gradient descent in low and high dimensions |
| 2026-01-29 | Thu | Stochastic gradient descent continued: neural networks |
| 2026-02-03 | Tue | Neural networks, transformers, attention |
| 2026-02-05 | Thu | Neural networks, transformers, attention |
| 2026-02-10 | Tue | Transformers and interacting particle systems |
| 2026-02-12 | Thu | Prompting as conditioning; RLHF |
| 2026-02-17 | Tue | Influence functions in LLMs; superconcentration; propagation of chaos |
| 2026-02-19 | Thu | Interlude: graph representation learning and spectral clustering |
| 2026-02-24 | Tue | Interlude: graph neural networks |
| 2026-02-26 | Thu | Latent variables I: K-means and EM |
| 2026-03-03 | Tue | Latent variables II: variational inference & ELBO; score matching |
| 2026-03-05 | Thu | GANs and autoencoders |
| 2026-03-10 | Tue | GANs and autoencoders II |
| 2026-03-12 | Thu | Diffusion models I |
